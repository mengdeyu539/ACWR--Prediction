{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkWgEXcvHahV",
    "outputId": "5ceeafc3-ae29-4521-ff8a-e6a32ed73325"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "from tabpfn_time_series import TabPFNTimeSeriesPredictor, TabPFNMode\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import plot_pred_and_actual_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics():\n",
    "    y_true = df_test.target.copy()\n",
    "    y_pred = pred.target.copy()\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return mae, mse, rmse, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qc1mHevjHS_4"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from autogluon.timeseries import TimeSeriesDataFrame\n",
    "\n",
    "from tabpfn_time_series.data_preparation import to_gluonts_univariate, generate_test_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "files = os.listdir('timeseries/')\n",
    "def categorize_acwr(acwr):\n",
    "    if acwr < 0.8:\n",
    "        return 0\n",
    "    elif 0.8 <= acwr < 1.5:\n",
    "        return 1\n",
    "    elif acwr > 1.5:\n",
    "        return 2\n",
    "datas = pd.DataFrame()\n",
    "file_names = []\n",
    "datass = []\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join('timeseries', file))\n",
    "        df['player_id'] = os.path.splitext(file)[0] *len(df)\n",
    "        df = df.drop_duplicates(subset='Date', ignore_index=True)\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%d.%m.%Y', dayfirst=True)  \n",
    "        df = df.sort_values(by='Date', ascending=True).reset_index(drop=True)\n",
    "\n",
    "        df.columns = df.columns.str.upper()\n",
    "        datas = df.rename(columns={'DATE': 'timestamp'})\n",
    "        #datas = pd.concat([datas, df], axis=0, ignore_index=True)\n",
    "\n",
    "        datas = datas.dropna(subset='ACWR')\n",
    "        datas = datas.dropna(subset='TOTAL_DISTANCE')\n",
    "\n",
    "        datas['ACWR_Risk'] = datas['ACWR'].apply(categorize_acwr)\n",
    "        datas = datas.dropna(subset='ACWR_Risk')\n",
    "        ACWR_Risk = datas['ACWR_Risk'].values\n",
    "        injury_risk = datas['ACWR'].values\n",
    "        datas = datas.drop(['CTL28', 'CTL42','ATL', 'PLAYER_ID','ACWR_Risk'],axis=1)\n",
    "        datas = datas.rename(columns={'ACWR': 'target'})\n",
    "        datas = datas.reset_index(drop=True)\n",
    "        datas[\"item_id\"] = 0 \n",
    "        datas.set_index([\"item_id\", \"timestamp\"], inplace=True)  \n",
    "        datas.item_ids = datas.index.get_level_values('item_id').unique()\n",
    "        value_column = datas.pop('target')\n",
    "        datas['target'] = value_column\n",
    "        datas = datas[cos.cols.tolist()]\n",
    "        #datas = datas[datas.columns[13:]]\n",
    "        if len(datas)>80:\n",
    "            file_names.append(file.replace('.csv','')) \t\n",
    "            datass.append(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, maes, mses, rmses, r2s = [], [], [], [], []\n",
    "accuracys, precisions, recalls, f1s = [], [], [], []\n",
    "\n",
    "for i in range(len(datass)):\n",
    "    data_2020 = datass[i][datass[i].index.get_level_values('timestamp').year == 2020]\n",
    "    train_size = int(len(data_2020) * 0.8)\n",
    "    df_train, df_test = data_2020.iloc[:train_size], data_2020.iloc[train_size:]\n",
    "    df_train.item_ids = df_train.index.get_level_values('item_id').unique()\n",
    "    df_test.item_ids = df_test.index.get_level_values('item_id').unique()\n",
    "    df_test_na = df_test.copy()\n",
    "    df_test_na.ACWR = np.nan\n",
    "    df_test_na.item_ids = df_test_na.index.get_level_values('item_id').unique()\n",
    "\n",
    "    predictor = TabPFNTimeSeriesPredictor(\n",
    "        tabpfn_mode=TabPFNMode.LOCAL,\n",
    "    )\n",
    "    if len(df_test)>0 and len(df_train):\n",
    "        pred = predictor.predict(df_train, df_test_na)\n",
    "    \n",
    "\n",
    "        plot_pred_and_actual_ts(\n",
    "            train=df_train,\n",
    "            test=df_test,\n",
    "            pred=pred,\n",
    "            save_path=f'figures/{file_names[i]}_2020.png'\n",
    "        )\n",
    "        mae, mse, rmse, r2 = metrics()\n",
    "        ids.append(f'{file_names[i]}_2020')\n",
    "        maes.append(mae)\n",
    "        mses.append(mse)\n",
    "        rmses.append(rmse)\n",
    "        r2s.append(r2)\n",
    "        pred['ACWR_RISK'] = pred['target'].apply(categorize_acwr)\n",
    "        df_test['ACWR_RISK'] = df_test['target'].apply(categorize_acwr)\n",
    "        y_true = df_test.ACWR_RISK.copy()\n",
    "        y_pred = pred.ACWR_RISK.copy()\n",
    "\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "        accuracys.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "        print(f\"ðŸ”¹ Accuracyï¼ˆå‡†ç¡®çŽ‡ï¼‰: {accuracy:.4f}\")\n",
    "        print(f\"ðŸ”¹ Precisionï¼ˆç²¾ç¡®çŽ‡ï¼‰: {precision:.4f}\")\n",
    "        print(f\"ðŸ”¹ Recallï¼ˆå¬å›žçŽ‡ï¼‰: {recall:.4f}\")\n",
    "        print(f\"ðŸ”¹ F1 Score: {f1:.4f}\\n\")\n",
    "        print(\"ðŸ”¹ Classification Report:\\n\", report)\n",
    "        print(\"ðŸ”¹ Confusion Matrix:\\n\", conf_matrix)\n",
    "        \n",
    "    data_2021 = datass[i][datass[i].index.get_level_values('timestamp').year == 2021]\n",
    "    train_size = int(len(data_2021) * 0.8)\n",
    "    df_train, df_test = data_2021.iloc[:train_size], data_2021.iloc[train_size:]\n",
    "    df_train.item_ids = df_train.index.get_level_values('item_id').unique()\n",
    "    df_test.item_ids = df_test.index.get_level_values('item_id').unique()\n",
    "    df_test_na = df_test.copy()\n",
    "    df_test_na.ACWR = np.nan\n",
    "    df_test_na.item_ids = df_test_na.index.get_level_values('item_id').unique()\n",
    "\n",
    "    if len(df_test)>0 and len(df_train):\n",
    "        pred = predictor.predict(df_train, df_test_na)\n",
    "    \n",
    "\n",
    "        plot_pred_and_actual_ts(\n",
    "            train=df_train,\n",
    "            test=df_test,\n",
    "            pred=pred,\n",
    "            save_path=f'figures/{file_names[i]}_2021.png'\n",
    "        )\n",
    "        mae, mse, rmse, r2 = metrics()\n",
    "        ids.append(f'{file_names[i]}_2021')\n",
    "        maes.append(mae)\n",
    "        mses.append(mse)\n",
    "        rmses.append(rmse)\n",
    "        r2s.append(r2)\n",
    "        pred['ACWR_RISK'] = pred['target'].apply(categorize_acwr)\n",
    "        df_test['ACWR_RISK'] = df_test['target'].apply(categorize_acwr)\n",
    "        y_true = df_test.ACWR_RISK.copy()\n",
    "        y_pred = pred.ACWR_RISK.copy()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "        accuracys.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "        print(f\"ðŸ”¹ Accuracyï¼ˆå‡†ç¡®çŽ‡ï¼‰: {accuracy:.4f}\")\n",
    "        print(f\"ðŸ”¹ Precisionï¼ˆç²¾ç¡®çŽ‡ï¼‰: {precision:.4f}\")\n",
    "        print(f\"ðŸ”¹ Recallï¼ˆå¬å›žçŽ‡ï¼‰: {recall:.4f}\")\n",
    "        print(f\"ðŸ”¹ F1 Score: {f1:.4f}\\n\")\n",
    "        print(\"ðŸ”¹ Classification Report:\\n\", report)\n",
    "        print(\"ðŸ”¹ Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_gi5gDFHS_5",
    "outputId": "c710a7a3-1b2b-4253-8071-fc5754d909ae"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'ID': ids,\n",
    "    'MAE': maes,\n",
    "    'MSE': mses,\n",
    "    'RMSE': rmses,\n",
    "    'R2': r2s,\n",
    "    'accuracy': accuracys,\n",
    "    'precision': precisions,\n",
    "    'recall': recalls,\n",
    "    'f1': f1s\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv('result_corr1.csv')\n",
    "\n",
    "print(\"Data saved to 'resultss.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "FCxt23reHS_5",
    "outputId": "5f639a2c-f5a8-4173-8800-32e4672e4cc3"
   },
   "outputs": [],
   "source": [
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = []\n",
    "for i in range(len(datass)):\n",
    "    dfs = datass[i].dropna()\n",
    "    corr_matrix = dfs.corr().fillna(0)\n",
    "    corrs.append(corr_matrix.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_stack = np.vstack(corrs)\n",
    "mean_values = np.mean(arr_stack, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = pd.DataFrame({'cols':datass[1].columns,'mean':mean_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = cos[cos['mean'].abs()>0.05]\n",
    "cos.cols.tolist()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
